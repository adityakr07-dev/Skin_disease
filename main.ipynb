{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16701a43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings with clear justification (avoid blanket silencing)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    category=UserWarning,  # Only target UserWarnings (not DeprecationWarning, etc.)\n",
    "    module=\"langchain\",    # Optional: Limit to specific module to avoid hiding unrelated issues\n",
    "    message=\".*regex_pattern.*\"  # Optional: Filter by message regex for precision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33206eb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Installing Python dependencies for a LangChain-based project with version pinning for reproducibility.\n",
    "# Usage: Run this script in a fresh virtual environment to avoid conflicts.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Defining dependencies with recommended versions \n",
    "REQUIRED_PACKAGES = [\n",
    "    \"langchain==0.1.11\",         # Framework for LLM applications\n",
    "    \"langgraph==0.0.22\",         # For building stateful, multi-actor workflows\n",
    "    \"cassio==0.1.10\",             # Cassandra DB integration for LangChain\n",
    "    \"langchain-community==0.0.28\", # Community-contributed LangChain integrations\n",
    "    \"openai==1.12.0\",            # Official OpenAI API client\n",
    "    \"wikipedia==1.4.0\",          # Wikipedia API wrapper for retrieval\n",
    "    \"tensorflow==2.15.0\",        # ML framework (optional: replace with `tensorflow-cpu` if no GPU)\n",
    "    \"langchain-groq==0.1.2\",     # Groq API integration for high-speed LLMs\n",
    "    \"tiktoken==0.6.0\",           # Fast BPE tokenizer for OpenAI models\n",
    "]\n",
    "\n",
    "def install_dependencies():\n",
    "    \"\"\"Install all required packages with pinned versions.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"pip\", \"install\"] + REQUIRED_PACKAGES, check=True)\n",
    "        print(\"✅ Dependencies installed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Installation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb310caf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Third-Party General Imports\n",
    "import numpy as np  # Numerical computing\n",
    "from PIL import Image  # Image processing\n",
    "import matplotlib.pyplot as plt  # Data visualization\n",
    "import geocoder  # Location services (consider alternatives like Google Maps API for production)\n",
    "import wikipedia  # Wikipedia API (handle with rate-limiting in production)\n",
    "\n",
    "# Machine Learning & Deep Learning\n",
    "import tensorflow as tf  # Standard alias for TensorFlow\n",
    "from tensorflow.keras.models import load_model  # Model loading\n",
    "from tensorflow.keras.preprocessing.image import img_to_array  # Image preprocessing\n",
    "\n",
    "# LangChain Core Components\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Text processing\n",
    "from langchain.chains import RetrievalQA  # Question-answering chain\n",
    "from langchain.chat_models import ChatOpenAI  # LLM providers (OpenAI + Groq)\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Vector Stores & Embeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings  # OpenAI embeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # HF embeddings (alternative)\n",
    "from langchain.vectorstores import AstraDB, Cassandra  # Database integrations\n",
    "\n",
    "# Document Loaders (Community Maintained)\n",
    "from langchain_community.document_loaders import WebBaseLoader  # Web content loader\n",
    "\n",
    "# Cassandra-specific (only import if needed)\n",
    "import cassio  # Cassandra driver for LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66754f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "# Image dimensions expected by the model (width, height)\n",
    "IMAGE_SIZE = (128, 128)  \n",
    "\n",
    "# List of possible skin disease classes the model can predict\n",
    "# Ordered to match model's output layer indices\n",
    "class_names = [\"Acne\", \"Actinic Keratosis\", \"Benign Tumors\", \"Bullous\", \"Candidiasis\", \"Drug Eruption\",\n",
    "               \"Eczema\", \"Infestations/Bites\", \"Lichen\", \"Lupus\", \"Moles\", \"Psoriasis\", \"Rosacea\",\n",
    "               \"Seborrheic Keratoses\", \"Skin Cancer\", \"Sun/Sunlight Damage\", \"Tinea\", \"Unknown/Normal\",\n",
    "               \"Vascular Tumors\", \"Vasculitis\", \"Vitiligo\", \"Warts\"]\n",
    "\n",
    "# --- Model Loading --- \n",
    "# Load pre-trained Keras model from specified file path\n",
    "# Note: Ensure the model architecture matches expected input/output dimensions\n",
    "model = load_model('/content/skindisease_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063500b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"\n",
    "    Prepares an image for model prediction by:\n",
    "    1. Loading the image\n",
    "    2. Converting to RGB format\n",
    "    3. Resizing to model's expected dimensions\n",
    "    4. Normalizing pixel values\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to input image file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (processed_image_array, original_image) \n",
    "               - processed_image_array: Normalized numpy array ready for model prediction\n",
    "               - original_image: PIL Image object for display purposes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open image and ensure RGB format (3 channels)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    # Resize to model's expected input dimensions (128x128)\n",
    "    img = img.resize(IMAGE_SIZE)\n",
    "    \n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = img_to_array(img)\n",
    "    \n",
    "    # Add batch dimension (changes shape from (h,w,c) to (1,h,w,c))\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "    # Normalize pixel values from [0,255] to [0,1] range\n",
    "    img_array /= 255.0\n",
    "    \n",
    "    return img_array, img  # Return both processed array and original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553d760",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get Top-3 Prediction from Model\n",
    "def predict_top_3(img_path):\n",
    "    \"\"\"\n",
    "    Makes top-3 disease predictions on an input image and displays results.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to the input image file\n",
    "        \n",
    "    Returns:\n",
    "        List of tuples: [(disease_name1, confidence_score1), \n",
    "                        (disease_name2, confidence_score2),\n",
    "                        (disease_name3, confidence_score3)]\n",
    "                        (sorted by highest confidence first)\n",
    "    \"\"\"\n",
    "    # Preprocess image - returns both array and original image\n",
    "    img_array, original_img = preprocess_image(img_path)\n",
    "    \n",
    "    # Get model predictions (returns probability distribution)\n",
    "    predictions = model.predict(img_array)[0]  # [0] gets first (and only) batch item\n",
    "    \n",
    "    # Get indices of top 3 predictions (sorted descending)\n",
    "    top_indices = predictions.argsort()[-3:][::-1]\n",
    "    \n",
    "    # Map indices to class names and confidence scores\n",
    "    top_diseases = [(class_names[idx], predictions[idx]) for idx in top_indices]\n",
    "    \n",
    "    # Display the image with top prediction as title\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(f\"Top Prediction: {top_diseases[0][0]}\")\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "    \n",
    "    return top_diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb31313",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Cassandra/AstraDB connection\n",
    "# Security Critical: This establishes a persistent database connection with the provided credentials\n",
    "# Recommended safeguards:\n",
    "# 1. Validate credentials exist before calling (add checks in production)\n",
    "# 2. Ensure this only runs once (duplicate inits may cause connection leaks)\n",
    "# 3. In production, wrap in try-catch to handle connection failures gracefully\n",
    "# 4. Consider adding connection timeout parameters for production use\n",
    "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a5065",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Load and insert disease info into AstraDB ---\n",
    "def load_and_insert_disease_info():\n",
    "    \"\"\"Loads skin disease information from web sources, processes it, and stores in AstraDB vector store.\"\"\"\n",
    "    \n",
    "    # Initialize HuggingFace embeddings model for text vectorization\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # URLs with disease info (currently single GitHub README - consider adding more sources)\n",
    "    urls = [\n",
    "        \"https://github.com/adityakr07-dev/Skin_disease/blob/main/README.md\",\n",
    "    ]\n",
    "    \n",
    "    # Load documents from all URLs (returns list of lists)\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    \n",
    "    # Flatten the list of lists into single document list\n",
    "    docs = [doc for sublist in docs for doc in sublist]  # flatten\n",
    "\n",
    "    # Initialize text splitter for chunking documents\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    \n",
    "    # Split documents into manageable chunks\n",
    "    doc_splits = splitter.split_documents(docs)\n",
    "\n",
    "    # Initialize Cassandra/AstraDB vector store\n",
    "    vectordb = Cassandra(\n",
    "        embedding=embedding,          # Embedding model to use\n",
    "        table_name=\"skin_disease_info\",  # Target table name\n",
    "        session=None,                # Auto-initialized session\n",
    "        keyspace=None,               # Default keyspace\n",
    "    )\n",
    "    \n",
    "    # Insert all document chunks into vector database\n",
    "    vectordb.add_documents(doc_splits)\n",
    "    \n",
    "    # Confirm successful insertion\n",
    "    print(f\"✅ Inserted {len(doc_splits)} disease documents into AstraDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be6502",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Setup AstraDB retriever ---\n",
    "def setup_vectorstore():\n",
    "    \"\"\"\n",
    "    Configures and returns a retriever for the skin disease information vector store.\n",
    "    \n",
    "    Returns:\n",
    "        A retriever object configured to fetch the top 3 most relevant document chunks\n",
    "        from the AstraDB/Cassandra vector store.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize HuggingFace sentence transformer embeddings\n",
    "    # Using 'all-MiniLM-L6-v2' - a balance of speed and accuracy for general text\n",
    "    embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Connect to existing Cassandra/AstraDB vector store\n",
    "    # Note: Uses same table name as in load_and_insert_disease_info()\n",
    "    vectordb = Cassandra(\n",
    "        embedding=embedding,          # Embedding model for query encoding\n",
    "        table_name=\"skin_disease_info\",  # Must match previously created table\n",
    "        session=None,                # Uses default session initialization\n",
    "        keyspace=None,               # Uses default keyspace\n",
    "    )\n",
    "    \n",
    "    # Convert vector store to retriever interface\n",
    "    # Configured to return top 3 most relevant results (k=3)\n",
    "    return vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200b414d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Filter top diseases using symptom match ---\n",
    "def filter_candidates_by_symptoms(symptom_text, top_diseases, retriever):\n",
    "    \"\"\"\n",
    "    Filters predicted diseases by verifying symptom matches in the knowledge base.\n",
    "    \n",
    "    Args:\n",
    "        symptom_text: User-provided symptom description\n",
    "        top_diseases: List of (disease_name, confidence) tuples from model prediction\n",
    "        retriever: Vector store retriever for disease information\n",
    "        \n",
    "    Returns:\n",
    "        List of matching (disease_name, document) tuples where symptoms are confirmed\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    # Check each predicted disease against knowledge base\n",
    "    for disease, _ in top_diseases:\n",
    "        # Search for symptom+disease combination in vector store\n",
    "        docs = retriever.get_relevant_documents(symptom_text + f\" related to {disease}\")\n",
    "        \n",
    "        # Verify disease is actually mentioned in retrieved documents\n",
    "        for doc in docs:\n",
    "            if disease.lower() in doc.page_content.lower():\n",
    "                matches.append((disease, doc))\n",
    "                break  # Stop after first confirming document\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b6a20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Fallback Wikipedia summary ---\n",
    "def fetch_wikipedia_info(disease_name):\n",
    "    \"\"\"\n",
    "    Fetches a brief Wikipedia summary for a given disease as fallback information.\n",
    "    \n",
    "    Args:\n",
    "        disease_name: Name of the disease to look up\n",
    "        \n",
    "    Returns:\n",
    "        Either a 3-sentence summary from Wikipedia, or an error message if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to fetch Wikipedia summary (first 3 sentences)\n",
    "        return wikipedia.summary(disease_name, sentences=3)\n",
    "    except Exception:\n",
    "        # Graceful fallback if Wikipedia lookup fails\n",
    "        return f\"No Wikipedia information found for {disease_name}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21602b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to find disease names in text by matching against known class_names\n",
    "def extract_disease_names_from_text(text):\n",
    "    return [disease for disease in class_names if disease in text.lower()]\n",
    "\n",
    "class SkinDiseaseChatBot:\n",
    "    def __init__(self, image_path):\n",
    "        # Initialize with:\n",
    "        # - Top 3 predicted diseases from image model (lowercased)\n",
    "        # - Empty symptom accumulator\n",
    "        # - Pre-configured vectorstore retriever\n",
    "        # - Groq LLM with deterministic settings (temperature=0)\n",
    "        self.top_diseases = [disease.lower() for disease, _ in predict_top_3(image_path)]\n",
    "        self.accumulated_symptoms = \"\"\n",
    "        self.retriever = setup_vectorstore()\n",
    "        self.llm = ChatGroq(\n",
    "            model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "            groq_api_key=\"....\",  # Note: Should be properly configured in production\n",
    "            temperature=0  # For consistent responses\n",
    "        )\n",
    "        self.chain = RetrievalQA.from_chain_type(llm=self.llm, retriever=self.retriever)\n",
    "\n",
    "    def update_symptoms(self, new_symptoms):\n",
    "        # Append new symptoms and trigger diagnosis refinement\n",
    "        self.accumulated_symptoms += \" \" + new_symptoms.strip()\n",
    "        return self.refine_diagnosis()\n",
    "\n",
    "    def refine_diagnosis(self):\n",
    "        # Three-tier diagnosis approach:\n",
    "        # 1. High confidence: Match against top predicted diseases\n",
    "        # 2. Medium confidence: Match against other known diseases\n",
    "        # 3. Low confidence: LLM inference + Wikipedia fallback\n",
    "        results = []\n",
    "        matched_diseases = set()\n",
    "\n",
    "        # High confidence check (top predicted diseases)\n",
    "        relevant_docs = self.retriever.get_relevant_documents(self.accumulated_symptoms)\n",
    "        for doc in relevant_docs:\n",
    "            content = doc.page_content.lower()\n",
    "            for top_disease in self.top_diseases:\n",
    "                if top_disease in content:\n",
    "                    if top_disease not in matched_diseases:\n",
    "                        g = geocoder.ip('me')\n",
    "                        address_my=g.city+\",\"+g.state+\",\"+g.country\n",
    "                        refined = self.chain.run(f\"Given this info: {content}\\n\\nGive information about what are the symptoms and treatments of {top_disease}? and possible skin doctors near {address_my}. Give your asnwer in structured way.\")\n",
    "                        confidence=\"High Possibility\"\n",
    "                        results.append((top_disease.title(), confidence, refined))\n",
    "                        return results\n",
    "\n",
    "        # Medium confidence check (other known diseases)\n",
    "        for doc in relevant_docs:\n",
    "            content = doc.page_content.lower()\n",
    "            for disease in extract_disease_names_from_text(content):\n",
    "                disease = disease.lower()\n",
    "                if disease not in matched_diseases and disease not in self.top_diseases:\n",
    "                    g = geocoder.ip('me')\n",
    "                    address_my=g.city+\",\"+g.state+\",\"+g.country\n",
    "                    refined = self.chain.run(f\"Given this info: {content}\\n\\nGive information about symptoms and treatments of {disease}? and possible skin  doctors near {address_my}. Give your asnwer in structured way.\")\n",
    "                    confidence=\"Medium Possibility\"\n",
    "                    results.append((disease.title(), confidence, refined))\n",
    "                    return results\n",
    "\n",
    "        # Low confidence fallback (LLM + Wikipedia)\n",
    "        if not results:\n",
    "            guessed_disease = self.llm.invoke(\n",
    "                f\"Based on the following symptoms, what skin disease or any other disease is most likely?\\n\\nSymptoms: {self.accumulated_symptoms}\\n\\nOnly return the name of the disease.\"\n",
    "            ).content.strip()\n",
    "\n",
    "            wiki_info = fetch_wikipedia_info(guessed_disease)\n",
    "            confidence = \"Low Possibility\"\n",
    "\n",
    "            if wiki_info != f\"No Wikipedia info found for {guessed_disease}.\":\n",
    "                g = geocoder.ip('me')\n",
    "                address_my=g.city+\",\"+g.state+\",\"+g.country\n",
    "                refined_answer = self.llm.invoke(\n",
    "                    f\"{self.accumulated_symptoms}\\n\\nBased on this and the information below, give information about symptoms and treatments for {guessed_disease}?\\n\\n{wiki_info} . Give your asnwer in structured way.\"\n",
    "                )\n",
    "                results.append((guessed_disease, confidence, refined_answer))\n",
    "            else:\n",
    "                results.append((guessed_disease, confidence, \"No relevant information available.\"))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e8550",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize chatbot with sample skin disease image\n",
    "# Note: Image path should point to actual uploaded file\n",
    "bot = SkinDiseaseChatBot(\"/content/Actinic-Keratosis-01.jpg\")\n",
    "\n",
    "# --- First Interaction ---\n",
    "# Provide initial symptom (\"moles\") and get response\n",
    "response1 = bot.update_symptoms(\"heart pain and cough\")\n",
    "\n",
    "# Print formatted results (disease name, confidence, information)\n",
    "print(\"\\n=== First Diagnosis ===\")\n",
    "for disease, confidence, info in response1:\n",
    "    print(f\"\\nDisease: {disease}\")\n",
    "    print(f\"Confidence: {confidence}\")\n",
    "    print(f\"Information:\\n{info}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# --- Second Interaction --- \n",
    "# Refine diagnosis by asking follow-up question\n",
    "response2 = bot.update_symptoms(\"what disease do you think i have\")\n",
    "\n",
    "# Print refined results\n",
    "print(\"\\n=== Refined Diagnosis ===\")\n",
    "for disease, confidence, info in response2:\n",
    "    print(f\"\\nDisease: {disease}\")\n",
    "    print(f\"Confidence: {confidence}\")\n",
    "    print(f\"Information:\\n{info}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
